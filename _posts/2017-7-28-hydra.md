---
layout: post
title: Hydra - a 4-GPU Workstation for Machine Learning
tags: [GPU, CUDA, Linux, Machine Learning]
---

Hydra is a rack-mountable 4-GPU workstation for Machine Learning research, kindly provided to us by our partners at [Pacific Research Platform](http://prp.ucsd.edu/).<!-- more -->

* Table of Contents
{:toc}

## Hardware
* Two 8-core [Intel Xeon E5-2620 v4](https://ark.intel.com/products/92986/Intel-Xeon-Processor-E5-2620-v4-20M-Cache-2_10-GHz) processors @ 2.1 GHz
* Four [Nvidia GeForce GTX 1080 Ti](https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti/) Founder's Edition Graphics Cards
* 128GB DDR4 ECC/REG memory (8 x 16GB)
* Two 128GB [SATA DOMs](https://www.supermicro.com/products/nfo/SATADOM.cfm)
* One 3.2TB [Samsung PM1725a HHHL PCIe NVMe SSD](http://www.samsung.com/semiconductor/products/flash-storage/enterprise-ssd/)
* Eight 480GB [Samsung PM863a SATA SSDs](http://www.samsung.com/semiconductor/minisite/ssd/product/enterprise/pm863a.html) (installed in eight hot-swap Drive Bays)
* One [Mellanox ConnectX-4 Lx EN (MCX4131A-BCAT)](http://www.mellanox.com/page/products_dyn?product_family=219&) 40GbE Single Port QSFP28 Network Adapter
* Integrated Intel i350 Dual Port Gigabit Ethernet Controller
* Integrated IPMI 2.0 with Virtual Media over LAN and KVM-over-LAN Support
* Integrated ASPEED AST2400 BMC Graphics
* Supermicro [7048GR-TR](https://www.supermicro.com/products/system/4U/7048/SYS-7048GR-TR.cfm) 4U rack-mountable Tower chassis
* 2000W high efficiency (96%) redundant PSUs, Titanium Level

## Basic Software
1) Minimal installation of CentOS 7.3 in July 2017.

2) Disable Selinux by changing the following line in `/etc/selinux/config`:
{% highlight conf %}
SELINUX=enforcing
{% endhighlight %}
to:
{% highlight conf %}
SELINUX=disabled
{% endhighlight %}

3) After copying SSH keys to the host, disable password authentication of SSH by changing the following line in `/etc/ssh/sshd_config`:
{% highlight conf %}
PasswordAuthentication yes
{% endhighlight %}
to
{% highlight conf %}
PasswordAuthentication no
{% endhighlight %}

4) Disable GSSAPI authentication of SSH by changing the following line in `/etc/ssh/sshd_config`:
{% highlight conf %}
GSSAPIAuthentication yes
{% endhighlight %}
to:
{% highlight conf %}
GSSAPIAuthentication no
{% endhighlight %}

5) Update all packages:
{% highlight shell_session %}
# yum -y update
{% endhighlight %}

6) Reboot;

7) Remove the old kernel:
{% highlight shell_session %}
# yum erase -y kernel-3.10.0-514.el7.x86_64
{% endhighlight %}

8) Install Development Tools
{% highlight shell_session %}
# yum groupinstall -y "Development Tools"
{% endhighlight %}

9) Enable EPEL repository:
{% highlight shell_session %}
# yum install -y epel-release
{% endhighlight %}

10) Install DKMS, which is required by *CUDA* packages:
{% highlight shell_session %}
# yum install -y dkms
{% endhighlight %}

11) Install [Environment Modules](http://modules.sourceforge.net/):
{% highlight shell_session %}
# yum -y install environment-modules
{% endhighlight %}

## CUDA
Initially the open source nouveau driver is loaded by default:
{% highlight shell_session %}
# lsmod | grep nouveau
nouveau              1527946  0
video                  24400  1 nouveau
mxm_wmi                13021  1 nouveau
drm_kms_helper        146456  2 ast,nouveau
ttm                    93908  2 ast,nouveau
i2c_algo_bit           13413  3 ast,igb,nouveau
drm                   372540  5 ast,ttm,drm_kms_helper,nouveau
i2c_core               40756  8 ast,drm,igb,i2c_i801,ipmi_ssif,drm_kms_helper,i2c_algo_bit,nouveau
wmi                    19070  2 mxm_wmi,nouveau
{% endhighlight %}

Install [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads), using Installer Type `rpm(network)`:
{% highlight shell_session %}
# yum install -y https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-repo-rhel7-8.0.61-1.x86_64.rpm
# yum clean all
# yum install -y cuda
# reboot
{% endhighlight %}
This installs CUDA 8.0 at `/usr/local/cuda-8.0`; and also creates a symbolic link `/usr/local/cuda` pointing to that directory.

Now CUDA 8.0 is installed; and the *nouveau* driver is blacklisted with `/etc/modprobe.d/blacklist-nouveau.conf`.
{% highlight conf %}
# RPM Fusion blacklist for nouveau driver - you need to run as root:
# dracut -f /boot/initramfs-$(uname -r).img $(uname -r)
# if nouveau is loaded despite this file.
blacklist nouveau
{% endhighlight %}

Let's list all Nvidia GPUs:
{% highlight shell_session %}
# nvidia-smi -L
GPU 0: GeForce GTX 1080 Ti (UUID: GPU-9e5e4bb8-5810-69c5-c783-1759ac6cd0ce)
GPU 1: GeForce GTX 1080 Ti (UUID: GPU-3d046d75-24b5-b06f-5ec2-ba43aeb6b17f)
GPU 2: GeForce GTX 1080 Ti (UUID: GPU-be6407ea-08f6-06d4-732b-47b40896950c)
GPU 3: GeForce GTX 1080 Ti (UUID: GPU-41444e04-2b58-56a6-2a85-318f03017dd6)
{% endhighlight %}

We note in passing that NVIDIA CUDA Profile Tools Interface is installed at `/usr/local/cuda-8.0/extras/CUPTI`. This library provides advanced profiling support; and is required by current version (1.3) of TensorFlow.

## cuDNN
The current version (1.3) of TensorFlow requires [cuDNN](https://developer.nvidia.com/cudnn) v6.0. Download the tarball of [cuDNN v6.0](https://developer.nvidia.com/rdp/cudnn-download) for CUDA 8.0; and unpack it:
{% highlight shell_session %}
# tar tfz cudnn-8.0-linux-x64-v6.0.tgz
cuda/include/cudnn.h
cuda/lib64/libcudnn.so
cuda/lib64/libcudnn.so.6
cuda/lib64/libcudnn.so.6.0.21
cuda/lib64/libcudnn_static.a

# tar xvfz cudnn-8.0-linux-x64-v6.0.tgz -C /usr/local/
{% endhighlight %}
which places the header and the libraries in `/usr/local/cuda`, which is a symbolic link to `/usr/local/cuda-8.0`.

<p class="note"> The CUDA installation has added an entry <strong>cuda-8-0.conf</strong> in <em>/etc/ld.so.conf.d/</em>; so there is no need to alter the <em>LD_LIBRARY_PATH</em> environment variable.</p>
{% highlight shell_session %}
# cat /etc/ld.so.conf.d/cuda-8-0.conf
/usr/local/cuda-8.0/targets/x86_64-linux/lib
{% endhighlight %}

Refresh the shared library cache:
{% highlight shell_session %}
# ldconfig
{% endhighlight %}

## ZFS on SATA SSDs
Install the ZFS repository for RHEL/CentOS 7.3:
{% highlight shell_session %}
# yum install -y http://download.zfsonlinux.org/epel/zfs-release.el7_3.noarch.rpm
{% endhighlight %}

We'll install [kABI-tracking kmod ZFS package](https://github.com/zfsonlinux/zfs/wiki/RHEL-and-CentOS). Follow instructions in the wiki to modify `/etc/yum.repos.d/zfs.repo` then install `zfs`:
{% highlight shell_session %}
# yum install -y zfs
{% endhighlight %}
which installs `zfs` as well as its dependencies `kmod-spl`, `kmod-zfs`, `libzfs2`, `libzpool2` & `spl`.

Enable and start ZFS:
{% highlight shell_session %}
# systemctl preset zfs-import-cache zfs-import-scan zfs-mount zfs.target
# systemctl start zfs.target
{% endhighlight %}
**Note** there is a *preset* file for ZFS at `/usr/lib/systemd/system-preset/50-zfs.preset`:
{% highlight conf %}
# ZFS is enabled by default
enable zfs-import-cache.service
disable zfs-import-scan.service
enable zfs-mount.service
enable zfs-share.service
enable zfs-zed.service
enable zfs.target
{% endhighlight %}
Here we've only enabled `zfs-import-cache`, `zfs-import-scan` & `zfs-mount zfs.target`. But it's worthwhile looking into [zfs-zed (ZFS Even Daemon)](http://louwrentius.com/the-zfs-event-daemon-on-linux.html).

Find out the disk IDs of the eight 480GB Samsung PM863a SATA SSDs:
{% highlight shell_session %}
# ls -lh /dev/disk/by-id/
lrwxrwxrwx 1 root root  9 Jul 28 21:05 ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411211 -> ../../sde
lrwxrwxrwx 1 root root  9 Jul 28 21:05 ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411217 -> ../../sdf
lrwxrwxrwx 1 root root  9 Jul 28 21:05 ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411300 -> ../../sdh
lrwxrwxrwx 1 root root  9 Jul 28 21:05 ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411303 -> ../../sdb
lrwxrwxrwx 1 root root  9 Jul 28 21:05 ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411318 -> ../../sdc
lrwxrwxrwx 1 root root  9 Jul 28 21:05 ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411319 -> ../../sdd
lrwxrwxrwx 1 root root  9 Jul 28 21:05 ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411320 -> ../../sda
lrwxrwxrwx 1 root root  9 Jul 28 21:05 ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411321 -> ../../sdg
{% endhighlight %}

Use the following shell script, `mkzfs`, to create a ZFS on the eight 480GB Samsung PM863a SATA SSDs:
{% highlight plaintext %}
#!/bin/bash
/sbin/modprobe zfs

zpool create -f -m /home home -o ashift=12 raidz1 \
      ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411320 \
      ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411303 \
      ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411318 \
      ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411319

zpool add -f home -o ashift=12 raidz1 \
      ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411211 \
      ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411217 \
      ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411321 \
      ata-SAMSUNG_MZ7LM480HMHQ-00005_S2UJNX0J411300

zfs set recordsize=1024K home
zfs set checksum=fletcher4 home
zfs set atime=off home
{% endhighlight %}

This script will automatically create `/etc/zfs/zpool.cache`; so we don't need to run the following command as instructed in the [Arch Linux wiki article]((https://wiki.archlinux.org/index.php/ZFS)):
{% highlight plaintext %}
zpool set cachefile=/etc/zfs/zpool.cache <pool>
{% endhighlight %}

## XFS on NVMe SSD
Partition the NVMe SSD:
{% highlight shell_session %}
# parted /dev/nvme0n1
GNU Parted 3.1
Using /dev/nvme0n1
Welcome to GNU Parted! Type 'help' to view a list of commands.
(parted) mklabel gpt
Warning: The existing disk label on /dev/nvme0n1 will be destroyed and all data on this
disk will be lost. Do you want to continue?
Yes/No? Yes
(parted) unit s
(parted) print free
Model: Unknown (unknown)
Disk /dev/nvme0n1: 6251233968s
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start  End          Size         File system  Name  Flags
        34s    6251233934s  6251233901s  Free Space

(parted) mkpart primary 0% 100%
(parted) print free
Model: Unknown (unknown)
Disk /dev/nvme0n1: 6251233968s
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start        End          Size         File system  Name     Flags
        34s          2047s        2014s        Free Space
 1      2048s        6251233279s  6251231232s               primary
        6251233280s  6251233934s  655s         Free Space

(parted) quit
Information: You may need to update /etc/fstab.
{% endhighlight %}

Create XFS on the NVMe partition:
{% highlight shell_session %}
# mkfs.xfs /dev/nvme0n1p1
meta-data=/dev/nvme0n1p1         isize=512    agcount=4, agsize=195350976 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=781403904, imaxpct=5
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=381544, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

# xfs_admin -L scratch /dev/nvme0n1p1
writing all SBs
new label = "scratch"

# mkdir /scratch
# mount /dev/nvme0n1p1 /scratch
# chmod 1777 /scratch

# ls -ld /scratch
drwxrwxrwt 2 root root 6 Jul 28 21:38 /scratch
{% endhighlight %}

<p class="note">We have the sticky bit set.</p>

Find out the UUID of the NVMe partition:
{% highlight shell_session %}
# cd /dev/disk/by-uuid/
# ls -l
lrwxrwxrwx 1 root root 15 Jul 28 21:39 f67abe6c-42db-4006-aece-ea87b89b8eaf -> ../../nvme0n1p1
{% endhighlight %}

Append the following line to `/etc/fstab` so that the partition will be automatically mounted on startup:
{% highlight conf %}
UUID=f67abe6c-42db-4006-aece-ea87b89b8eaf /scratch                xfs     defaults        0 0
{% endhighlight %}

## Upgrading to CentOS 7.4
We upgraded the OS to CentOS 7.4 in November 2017:
{% highlight shell_session %}
# yum -y update
# reboot
{% endhighlight %}

1) After reboot, the kernel has been upgraded to `3.10.0-693.5.2.el7.x86_64` (from `3.10.0-514.26.2.el7.x86_64`):  
{% highlight shell_session %}
# uname -r
3.10.0-693.5.2.el7.x86_64
{% endhighlight %}

2) The *nvidia* driver has been upgraded to `384.81` (from `384.66`):
{% highlight shell_session %}
# nvidia-smi
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
{% endhighlight %}

3) CUDA 9.0 is installed; and `/usr/local/cuda` now points to `cuda-9.0`. **Note** CUDA 8.0 is not removed; it is still kept at `/usr/local/cuda-8.0`.

4) The CUDA 9.0 installation has added an entry `cuda-9-0.conf` in `/etc/ld.so.conf.d/`:
{% highlight shell_session %}
# cat /etc/ld.so.conf.d/cuda-9-0.conf
/usr/local/cuda-9.0/targets/x86_64-linux/lib
{% endhighlight %}

### ZFS
But ZFS stopped working! Let's fix it.

Remove old `zfs-release`, which is for RHEL/CentOS 7.3:
{% highlight shell_session %}
# rpm -q zfs-release
zfs-release-1-4.el7_3.centos.noarch

# yum erase zfs-release
{% endhighlight %}

Install zfs-release for EL7.4:
{% highlight shell_session %}
# yum -y install http://download.zfsonlinux.org/epel/zfs-release.el7_4.noarch.rpm
{% endhighlight %}

We'll again use the [kABI-tracking kmod ZFS package](https://github.com/zfsonlinux/zfs/wiki/RHEL-and-CentOS). Follow instructions in the wiki to modify `/etc/yum.repos.d/zfs.repo` then update to install the latest *zfs* packages:
{% highlight shell_session %}
# yum clean all
# yum update
{% endhighlight %}

Restart zfs.target:
{% highlight shell_session %}
# systemctl restart zfs.target
{% endhighlight %}

Now ZFS is mounted:
{% highlight shell_session %}
# df -h /home
Filesystem      Size  Used Avail Use% Mounted on
home            2.5T  379G  2.1T  16% /home
{% endhighlight %}

ZFS has been upgraded to `0.7.3-1` (from `0.7.1-1`):
{% highlight shell_session %}
# cat /sys/module/zfs/version
0.7.3-1
{% endhighlight %}

### CephFS
CephFS was not mounted either! As it turns out, the systemd unit file `/usr/lib/systemd/system/ceph-fuse@.service` has been overwritten. We had to modify it again by following steps in the post [Mounting a Subdirectory of CephFS on a CentOS 7 Client]({{ site.baseurl }}{% post_url 2017-9-25-ceph-fuse %}); then it works again!

### cuDNN
Download the tarball of [cuDNN v7.0](https://developer.nvidia.com/rdp/cudnn-download) for CUDA 9.0; and unpack it:
{% highlight shell_session %}
# tar tfz cudnn-9.0-linux-x64-v7.tgz
cuda/include/cudnn.h
cuda/NVIDIA_SLA_cuDNN_Support.txt
cuda/lib64/libcudnn.so
cuda/lib64/libcudnn.so.7
cuda/lib64/libcudnn.so.7.0.4
cuda/lib64/libcudnn_static.a

# tar xvfz cudnn-9.0-linux-x64-v7.0.tgz -C /usr/local/
{% endhighlight %}

Refresh the shared library cache:
{% highlight shell_session %}
# ldconfig
{% endhighlight %}

## Upgrading to CentOS 7.5
We upgraded the OS to CentOS 7.5 in July 2018:
{% highlight shell_session %}
# yum -y update
# reboot
{% endhighlight %}

1) After reboot, the kernel has been upgraded to `3.10.0-862.9.1.el7.x86_64`:
{% highlight shell_session %}
# uname -r
3.10.0-862.9.1.el7.x86_64
{% endhighlight %}

2) The *nvidia* driver has been upgraded to `396.37`:
{% highlight shell_session %}
# nvidia-smi
| NVIDIA-SMI 396.37                 Driver Version: 396.37                    |
{% endhighlight %}

3) CUDA 9.2 is installed; and `/usr/local/cuda` now points to `cuda-9.2`. **Note** CUDA 9.0 is not removed; it is still kept at `/usr/local/cuda-9.0`.

4) The CUDA 9.2 installation has added an entry `cuda-9-2.conf` in `/etc/ld.so.conf.d/`:
{% highlight shell_session %}
# cat /etc/ld.so.conf.d/cuda-9-2.conf
/usr/local/cuda-9.2/targets/x86_64-linux/lib
{% endhighlight %}

### ZFS
But ZFS again stopped working! Let's fix it.

Remove old `zfs-release`, which is for RHEL/CentOS 7.3:
{% highlight shell_session %}
# rpm -q zfs-release
zfs-release-1-5.el7_4.noarch

# yum erase zfs-release
{% endhighlight %}

Install zfs-release for EL7.4:
{% highlight shell_session %}
# yum -y install http://download.zfsonlinux.org/epel/zfs-release.el7_5.noarch.rpm
{% endhighlight %}

We'll again use the [kABI-tracking kmod ZFS package](https://github.com/zfsonlinux/zfs/wiki/RHEL-and-CentOS). Follow instructions in the wiki to modify `/etc/yum.repos.d/zfs.repo` then update to install the latest *zfs* packages:
{% highlight shell_session %}
# yum clean all
# yum update
{% endhighlight %}

Restart zfs.target:
{% highlight shell_session %}
# systemctl restart zfs.target
{% endhighlight %}

Now ZFS is mounted:
{% highlight shell_session %}
# df -h /home
Filesystem      Size  Used Avail Use% Mounted on
home            2.5T  379G  2.1T  16% /home
{% endhighlight %}

ZFS has been upgraded to `0.7.9-1` (from `0.7.3-1`):
{% highlight shell_session %}
# cat /sys/module/zfs/version
0.7.9-1
{% endhighlight %}

### CephFS
CephFS was not mounted either! As it turns out, the systemd unit file `/usr/lib/systemd/system/ceph-fuse@.service` has been overwritten. We had to modify it again by following steps in the post [Mounting a Subdirectory of CephFS on a CentOS 7 Client]({{ site.baseurl }}{% post_url 2017-9-25-ceph-fuse %}); then it works again!

### cuDNN
Download the tarball of [cuDNN v7.1](https://developer.nvidia.com/rdp/cudnn-download) for CUDA 9.2; and unpack it:
{% highlight shell_session %}
# tar tfz cudnn-9.2-linux-x64-v7.1.tgz
cuda/include/cudnn.h
cuda/NVIDIA_SLA_cuDNN_Support.txt
cuda/lib64/libcudnn.so
cuda/lib64/libcudnn.so.7
cuda/lib64/libcudnn.so.7.1.4
cuda/lib64/libcudnn_static.a

# tar xvfz cudnn-9.2-linux-x64-v7.1.tgz -C /usr/local/
{% endhighlight %}

Refresh the shared library cache:
{% highlight shell_session %}
# ldconfig
{% endhighlight %}
